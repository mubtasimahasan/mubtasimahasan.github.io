---
---

@misc{ahasan2024dmcodecdistillingmultimodalrepresentations,
      title={DM-Codec: Distilling Multimodal Representations for Speech Tokenization}, 
      author={Md Mubtasim Ahasan and Md Fahim and Tasnim Mohiuddin and A K M Mahbubur Rahman and Aman Chadha and Tariq Iqbal and M Ashraful Amin and Md Mofijul Islam and Amin Ahsan Ali},
      year={2025},
      eprint={2410.15017},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.15017}, 
      abstract={Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset.},
      html={https://arxiv.org/abs/2410.15017}, 
      pdf={https://arxiv.org/pdf/2410.15017},
      selected={true},
      abbr={EMNLP 2025},
      preview={dmcodec.png},
      code={https://github.com/mubtasimahasan/DM-Codec}
}

@misc{ahasan2025fusecodecsemanticcontextualfusionsupervision,
      title={FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs}, 
      author={Md Mubtasim Ahasan and Rafat Hasan Khan and Tasnim Mohiuddin and Aman Chadha and Tariq Iqbal and M Ashraful Amin and Amin Ahsan Ali and Md Mofijul Islam and A K M Mahbubur Rahman},
      year={2025},
      eprint={2509.11425},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2509.11425},      
      abstract={Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology's applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks.},
      html={https://arxiv.org/abs/2509.11425}, 
      pdf={https://arxiv.org/pdf/2509.11425},
      selected={true},
      abbr={Preprint},
      preview={fusecodec.png},
      code={https://github.com/mubtasimahasan/FuseCodec} 
}
